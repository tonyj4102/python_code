{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Data Analysis library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# more on the web\n",
    "http://blog.yhat.com/posts/data-science-in-python-tutorial.html\n",
    "\n",
    "http://www.datacommunitydc.org/blog/2013/07/python-for-data-analysis-the-landscape-of-tutorials\n",
    "IPython\n",
    "The IPython tutorial\n",
    "Fernando Perez's talk on IPython (and video)\n",
    "PyCon 2012 tutorial\n",
    "Interesting IPython notebooks\n",
    "IPython notebook examples\n",
    "Python Data Analysis Library (pandas)\n",
    "\n",
    "The 10-minute introduction to pandas\n",
    "The pandas cookbook\n",
    "2012 PyData Workshop\n",
    "The pandas documentation\n",
    "Randal Olson's tutorial\n",
    "Wes McKinney's tutorials 1 and 2 on Kaggle.\n",
    "Hernan Rojas' tutorial\n",
    "Tutorials on financial data and time series using pandas\n",
    "Scikit-learn\n",
    "\n",
    "2012 PyData Workshop\n",
    "Official scikit-learn tutorial\n",
    "Jacob VanderPlas' tutorial\n",
    "PyCon 2013 tutorial on advanced machine learning with scikit-learn\n",
    "More scikit-learn tutorials.\n",
    "Matplotlib\n",
    "\n",
    "Official tutorial\n",
    "N.P. Rougier's tutorial from EuroSciPy 2012\n",
    "Jake VanderPlas' tutorial from PyData NYC 2012\n",
    "John Hunter's Advanced Matplotlib Tutorial from PyData 2012\n",
    "A tutorial from Scigraph.\n",
    "Sympy\n",
    "\n",
    "Official tutorial\n",
    "SciPy 2013 presentations\n",
    "Numpy and Scipy\n",
    "\n",
    "The Guide to Numpy\n",
    "M. Scott Shell's Introduction to Numpy and Scipy\n",
    "Databases from Python\n",
    "\n",
    "SQLite\n",
    "MySQL\n",
    "PostgreSQL\n",
    "Books\n",
    "\n",
    "NLT\n",
    "http://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read from CSV\n",
    "train=read.csv(\"train.csv\")\n",
    "test=read.csv(\"santander_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Download data files\n",
    "!wget --no-check-certificate --output-document countries_metadata.csv https://ibm.box.com/shared/static/qh3o86mpij17ot7anydcmbwt41lwxvln.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## read & write feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preview files from command line\n",
    "!cat data/microbiome.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read csv using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_outofschool = pd.read_csv('outofschoolchildren.csv', skiprows = 2)\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "expedia_pred3 = pd.read_csv('preds3rf.csv')\n",
    "mb_file = pd.ExcelFile('data/microbiome/MID1.xls')  #Parse worksheet mb1 = mb_file.parse(\"Sheet 1\", header=None);\n",
    "#ormb2 = pd.read_excel('data/microbiome/MID2.xls', sheetname='Sheet 1', header=None)\n",
    "mb1.columns = [\"Taxon\", \"Count\"]\n",
    "\n",
    "\n",
    "#options\n",
    "sep=',')\n",
    "index_col=['Taxon','Patient']) # heirachical index\n",
    "skiprows=[3,4,6]).head()\n",
    "nrows=4)\n",
    ".head(20)\n",
    "#chunks\n",
    "data_chunks = pd.read_csv(\"data/microbiome.csv\", chunksize=15)\n",
    "mean_tissue = {chunk.Taxon[0]:chunk.Tissue.mean() for chunk in data_chunks\n",
    "mean_tissue\n",
    "               \n",
    "               \n",
    "#write\n",
    "mb.to_csv(\"mb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-58-1286198d45a6>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-1286198d45a6>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    print \"Dataset size: %d x %d\"%(br_paintings.shape)\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## read form zip\n",
    "# un-zip the paintings file\n",
    "import zipfile\n",
    "zipper = zipfile.ZipFile('./data/bob_ross/bob_ross_paintings.npy.zip')\n",
    "zipper.extractall('./data/bob_ross/')\n",
    "\n",
    "# load the 403 x 360,000 matrix\n",
    "br_paintings = np.load(open('./data/bob_ross/bob_ross_paintings.npy','rb'))\n",
    "print \"Dataset size: %d x %d\"%(br_paintings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mining the web\n",
    "http://nbviewer.jupyter.org/github/ptwobrussell/Mining-the-Social-Web-2nd-Edition/tree/master/ipynb/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Methods - hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning methods - advantages, disadvantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Supervised\n",
    "\n",
    "|Algorithm/method | Subclass(Classification-(C)), Regression (R) | Problem | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "| Linear/non-linear/splines regression  |  | Predict/estimate a continuous outcome /$/Rates  | Assumes linear relationship, N/A’s must be removed |  | doesn't work well, with lots of discrete dirvers (eg. zip code)can't handle discontinuous relationships (step functions)doesn't handle missing values well,' |\n",
    "| K-nearest neighbors   |  |  |  | capable of making very good metafeatures for ensemble |  |\n",
    "| Neural Networks   | C, R |  |  |  |  |\n",
    "| Deep Neural Networks   | C,R, Feature eng. |  |  |  |  |\n",
    "| Support Vector Machine   |  | Anomalie detection (one-class SVM) |  |  |  |\n",
    "| Random Forest   |  | avg. tress (bagging) from different samples - reduce variance |  | handles the overfitting problem you faced with decision trees, Better prediction accuracy than CART | -detects variable interactions, -many parameters to adjust, -Less interpretable than CART |\n",
    "| XGBoost  | Classification  |  | trees + boosting | Subsampling prevents overfitting, & reduces running time, great dealing with linear and non-linear features, also it can handle dense or sparse data, scalable, less prone to outliers |  |\n",
    "| Logistic Regression/LASSO/Ridge  | C  | Score data/ln (P(y=1)/1P(y=1)=b0+b1x1+b2x2..)  | assumes linear and additive relationships | robust with redundant varaibles, correlated varaibles  |cannot handle varaibles that affect outcome in a discontinuous way e.g. step function Does not give a simple explanation of how a decision is made, doesn't handle missing values well' |\n",
    "| Naive Bayes   |  |scoring  | good for pred. class, not probabilities; numerical variables have to be categorical, sensitive to correlated varaibles | Handles missing values well, robust to irrelevant varaibles, easy to score, resistant to overfitting, good for high dimensional problems |  |\n",
    "| Decition Trees   |  |deep tree is prob overfit, not good for outcomes that are dependent on many varaibles/overfitting  |  |  | doesn't handle missing values well' |\n",
    "| CART   |  |  |  | Interpretable-handle non-linear variables | Doesn’t assume linear model-may not work on small datasets\n",
    "| Descriminant Analysis | M/C  |  | continuous independent variables and a categorical dependent variable (i.e. the class label) |  |  |\n",
    "| One-vs-All   |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "Classification: \n",
    "Graph - score (pred. probabilites for actuals 0,1) vs count /look for good separation\n",
    "FPR=#false positives/all negatives\n",
    "TPR = #true positives/all positives\n",
    "AUC\n",
    "\n",
    "## Unsupervised\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "| Association Rules |  | market basket, recommender, usage patterns | confidence & lift |  | Exponential time complexity |\n",
    "| PCA  |  | Feature Selection  |  |  |  |\n",
    "| K-means Clustering   |  | varaibles houdl be on simiilar scales |  | Works with any data set size | Need to select the no. of clusters., doesn't handle categorical variables' |\n",
    "| Hierachical Clustering   |  |  |  |  | Cannot do large images i.eg. 67B disst.  |\n",
    "| Content filter clustiner  |  |  |  |  |  |\n",
    "| Bag of words/corpus   |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "## Ensemble learning\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "\n",
    "## Reinforcement learning\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "## Times Series\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "\n",
    "## Special\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "| Conjoint Analysis   |  |  |  |  |  |\n",
    "| Factor Analysis  |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Libraries source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Libraries comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"py_bnch2.png\">\n",
    "https://pythonhosted.org/milk/benchmarks.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"python_lib_bench.png\"> \n",
    "http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical & other methods - advantages, disadvantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Descriptive Analysis\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "| Count   | univariate/categorical |  |  |  |  |\n",
    "| Pie chart/bar chart     | univariate/categorical ||  |  |  |\n",
    "| min, max, median, mean , mode   | univariate/numerical |  |  |  |  |\n",
    "| range, quartiles, variance,standard devi. coef. var   | univariate/numerical |  |  |  |  |\n",
    "| SKewness, Kurtosis   | univariate/numerical |  |  |  |  |\n",
    "| Histogram, Boxplot    | univariate/numerical | |  |  |  |\n",
    "|    | univariate/numerical |  |  |  |  |\n",
    "|    | univariate/numerical |  |  |  |  |\n",
    "|  Chi-square test | Bivariate/Cat & Cat. |  |  |  |  |\n",
    "|  Bar-chart, 2-Y axis plot | Bivariate/Cat & Cat. |  |  |  |  |\n",
    "|  Correlation  | Bivariate/Num. & Num. |  |  |  |  |\n",
    "| Scatterplot   | Bivariate/Num. & Num. |  |  |  |  |\n",
    "|  Z-test, t-test, ANOVA  | Bivariate/Num & Cat. |  |  |  |  |\n",
    "|   Bar & line chart, 2-Y axis plot | Bivariate/Num & Cat. |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "##  Slicing & Dicing/Aggregate Analysis\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "##  Inferencial Statistics\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "| A/B Testing   |  |  |  |  |  |\n",
    "| ANOVA  |  |  |  |  |  |\n",
    "| Design of Experiements   |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "##  Probability/Bayesian \n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "##  Sampling\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|  Bootstrap  |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "\n",
    "\n",
    "##  Real-time analysis\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|  Fourier  |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization methods - advantages, disadvantages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|Algorithm/method | Subclass | Problem  | Assumptions/requirements | Advantages  | Disadvantages  | \n",
    "|-----------|---------|---------|---------|---------|---------|\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |\n",
    "|    |  |  |  |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C Compilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numba \n",
    "@python\n",
    "numba.__version__\n",
    "'0.25.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train.head())\n",
    "df_outofschool.tail(10)\n",
    ".describe() #explore a DataFrame using the \n",
    ".shape attribute of your DataFrame object. (ex. your_data.shape)\n",
    "df_outofschool.columns\n",
    "df_merged.info()\n",
    "counts.index\n",
    "baseball.sum()\n",
    "#summarize over rows instead of columns, which only makes sense in certain situations.\n",
    "extra_bases = baseball[['X2b','X3b','hr']].sum(axis=1)\n",
    "extra_bases.order(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## deeper profile & exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(df_outofschool.country_code.unique())\n",
    "\n",
    "#unique - columns with \"\" in the names\n",
    "#but how to access columns with \"\" in their names?\n",
    "#iloc\n",
    "len(df_merged.iloc[:,-5].unique())\n",
    "\n",
    "#is null\n",
    "bacteria2.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# expedia_train[\"hotel_cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c11ebad48792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#factors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#counts by factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# proportions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#factors-subet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#factors\n",
    "print(train[\"Survived\"].value_counts())  #counts by factor\n",
    "print(train[\"Survived\"].value_counts(normalize = True))   # proportions\n",
    "\n",
    "#factors-subet\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts()) # Females that survived vs Females that passed away\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True))  # proportion and subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-24252aa339e7>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-24252aa339e7>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    http://www.gis.usu.edu/~chrisg/python/2009/\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "http://www.gis.usu.edu/~chrisg/python/2009/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numpy for IDL users\n",
    "http://mathesaurus.sourceforge.net/idl-numpy.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate summary statistics across multiple columns, for example, correlation and covariance.\n",
    "baseball.hr.cov(baseball.X2b)\n",
    "\n",
    "#Correlations\n",
    "baseball.corr()\n",
    "\n",
    "#histogram\n",
    "segments.seg_length.hist(bins=500)\n",
    "segments.seg_length.apply(np.log).hist(bins=500) #apply log transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create/add new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-d9583d6dae0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create new variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0myour_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"new_var\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m   \u001b[0;31m# 10 for each observation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m  \u001b[0;31m#(i) create a new column, and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#(ii) provide the values for each observation (i.e., row) based on the age of the passenger.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'your_data' is not defined"
     ]
    }
   ],
   "source": [
    "# create new variables\n",
    "your_data[\"new_var\"] = 10   # 10 for each observation\n",
    " #(i) create a new column, and \n",
    "    #(ii) provide the values for each observation (i.e., row) based on the age of the passenger.\n",
    "#example    \n",
    "train[\"Child\"] = float('NaN')\n",
    "train.Child[train.Age < 18] = 1\n",
    "train.Child[train.Age >= 18] = 0\n",
    "print(train.Child)\n",
    "\n",
    "\n",
    "# Initialize a Survived column to 0\n",
    "test_one[\"Survived\"] = 0\n",
    "# Set Survived to 1 if Sex equals \"female\" and print the `Survived` column from `test_one`\n",
    "test_one[\"Survived\"][train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "\n",
    "#df_merged['percentage_male'] = 100- df_merged['percentage_female']\n",
    "\n",
    "\n",
    "#multiply\n",
    "data['month'] = ['Jan']*len(data)\n",
    "\n",
    "#number of empty fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[632, 1638, 569, 115, 433, 1130, 754, 555],\n",
    "                     'patient':[1, 1, 1, 1, 2, 2, 2, 2],\n",
    "                     'phylum':['Firmicutes', 'Proteobacteria', 'Actinobacteria', \n",
    "    'Bacteroidetes', 'Firmicutes', 'Proteobacteria', 'Actinobacteria', 'Bacteroidetes']})\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#rename column headers\n",
    "df_outofschool.rename(columns={'Unnamed: 1':'year'}, inplace= True)\n",
    "#change all columns\n",
    "df_outofschool.columns = ['country', 'year', 'empty', 'all_children', 'female', 'male', 'percentage_female']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shift up first column\n",
    "df_outofschool.country = df_outofschool.country.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e0c8ca5b0556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#drop a column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'empty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "#drop a column\n",
    "df_outofschool.drop('empty', axis=1, inplace= True); or baseball.drop(['ibb','hbp'], axis=1)\n",
    "#drop a rwo by index\n",
    "baseball.drop([89525, 89526])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SORT\n",
    "average_children_by_region.sort(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-408438c74822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#removing bottom rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3893\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#alternative methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df_outofschool.drop(df_outofschool.tail(7).index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "#remove rows \n",
    "#removing bottom rows\n",
    "df_outofschool.drop(df_outofschool.index[3893:3900], inplace= True)\n",
    "#alternative methods\n",
    "#df_outofschool.drop(df_outofschool.tail(7).index)\n",
    "#df_outofschool.drop(list(range(3894,3900)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-a62ac4128afb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fill names down to the next (replace NaN below wiht country names)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "# fill names down to the next (replace NaN below wiht country names)\n",
    "df_outofschool['country'] = df_outofschool['country'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#N/A's\n",
    "foo.isnull()\n",
    "bacteria2.dropna() #drop na\n",
    "bacteria2[bacteria2.notnull()] \n",
    "#drops a row when every field is a missing value.\n",
    "data.dropna(how='all')\n",
    "# how many values need to be present before a row is dropped via the thresh argument.\n",
    "data.ix[7, 'year'] = np.nan\n",
    "bacteria2.fillna(0)\n",
    "data.fillna({'year': 2013, 'treatment':2})\n",
    "#alter values in place \n",
    "_ = data.year.fillna(2013, inplace=True)\n",
    "data\n",
    "# Missing values can also be interpolated,\n",
    "bacteria2.fillna(method='bfill')\n",
    "bacteria2.fillna(bacteria2.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace cells with ..\n",
    "#fill the '..' cells with NaN\n",
    "df_outofschool.replace('..', np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd2912f4ed0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cleaning/imputing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Impute the Embarked variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#cleaning/imputing\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "#Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "test.Fare[152] = test.Fare.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5bcbfc9e3aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# country=AFG: Afghanistan      # -> country_code=AFG   country=Afganistan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#first look at the string values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "# split text into two columns\n",
    "# country=AFG: Afghanistan\t# -> country_code=AFG   country=Afganistan\n",
    "#first look at the string values\n",
    "df_outofschool.loc[0, 'country']\n",
    "df_outofschool.loc[0, 'country'].split(': ')\n",
    "df_outofschool.loc[0, 'country'].split(': ')[0]\n",
    "get_country_code = lambda string: string.split(': ')[0]\n",
    "#to look at what it does\n",
    "df_outofschool['country'].apply(get_country_code)\n",
    "df_outofschool['country_code'] =  df_outofschool['country'].apply(get_country_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove rows with no data\n",
    "df_merged.dropna(axis = 0, how = 'all', subset= df_merged.columns[2:6], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a762f2504acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# change data type -float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#selct by column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# change data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# change data type -float\n",
    "df_merged[df_merged.columns[2:6]].astype(float)   #selct by column name\n",
    "df_merged[df_merged.columns[2:6]] = df_merged[df_merged.columns[2:6]].astype(float)\n",
    "# change data type \n",
    "-int\n",
    "df_merged[df_merged.columns[1]] = df_merged[df_merged.columns[1]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rounding off decimal places\n",
    "np.round(pivot_year_region, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation-Munging/Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e485f8d984ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Convert the male and female groups to integer form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"male\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Sex\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"female\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# Set some Pandas options\n",
    "pd.set_option('display.notebook_repr_html', False)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#date & time\n",
    "from datetime import datetime\n",
    "from datetime import date, time\n",
    "from dateutil.parser import parse\n",
    "https://github.com/fonnesbeck/statistical-analysis-python-tutorial/blob/master/2.%20Data%20Wrangling%20with%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Ranking \n",
    "#Ranking does not re-arrange data, but instead returns an index that ranks each value relative to others in the Series.\n",
    "baseball.hr.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transformation\n",
    "#Convert the male and female groups to integer form\n",
    "train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "\n",
    "#Convert the Embarked classes to integer form\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "#Print the Sex and Embarked columns\n",
    "print(train[\"Sex\"])\n",
    "print(train[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discretization ==> numerical to categorical groups  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# binarize data  color=red,blue, red, gree ==> three columns (red, blue, green)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Data aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://github.com/fonnesbeck/statistical-analysis-python-tutorial/blob/master/2.%20Data%20Wrangling%20with%20Pandas.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Concatenation-appending rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.concatenate([np.random.random(5), np.random.random(5)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### enriching through joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making sure column join names match 1:1\n",
    "df_metadata.rename(columns={'Country Code' : 'country_code'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'http://i.stack.imgur.com/sKS1u.jpg', width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_outofschool, df_metadata, on= 'country_code', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add column to groupby\n",
    "by_country = df_merged.groupby('country')\n",
    "In [82]:\n",
    "\n",
    "type(by_country)\n",
    "#Out[82]:\n",
    "#pandas.core.groupby.DataFrameGroupBy\n",
    "\n",
    "# counts shown by the following group by data structure\n",
    "by_country.count()\n",
    "\n",
    "#show counts by selected columns\n",
    "by_country.count().iloc[:,1:5]\n",
    "\n",
    "\"\"\n",
    "by_year = df_merged.groupby('year')\n",
    "df_merged.dtypes\n",
    "#Out[105]\n",
    "country               object\n",
    "year                  object\n",
    "all_children          object\n",
    "\"\"\n",
    "df_merged.columns[2:6] # column names\n",
    "\n",
    "by_region['all_children'].sum()\n",
    "by_region['all_children'].mean()\n",
    "by_region['all_children'].max()\n",
    "#Region\n",
    "#East Asia & Pacific            1557369.0\n",
    "#Europe & Central Asia           397419.0\n",
    "#Latin America & Caribbean       801014.0\n",
    "\n",
    "#Group by Region then by year, value = mean of all_children\n",
    "df_merged.groupby(['Region', 'year'])['all_children'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3e68d5fbe881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Region'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "df_merged.groupby(['Region', 'year'])['all_children'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pivot table\n",
    "df_merged.pivot_table('all_children',\n",
    "                     index = 'year', columns = 'Region', aggfunc= 'mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged[df_merged['all_children'] == 1338154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration/Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Plots\n",
    "    # df.plot(kind=\"scatter\", x=\"weekday\", y=\"dayhour\")\n",
    "    \n",
    "###\n",
    "# individual feature in Seaborn through a boxplot\n",
    "sns.boxplot(x=\"weekday\", y=\"dayhour\", data=expedia_train)\n",
    "# extend this plot is adding a layer of individual points on top of\n",
    "ax = sns.stripplot(x=\"weekday\", y=\"dayhour\", data=expedia_train, jitter=True, edgecolor=\"gray\")\n",
    "\n",
    "#df[\"weekday\", \"dayhour\"].boxplot(by=\"year\", figsize=(12, 6))\n",
    "###\n",
    "\n",
    "# individual feature in Seaborn through a boxplot\n",
    "sns.boxplot(x=\"hotel_cluster\", y=\"srch_destination_id\", data=expedia_train)\n",
    "# extend this plot is adding a layer of individual points on top of\n",
    "ax = sns.stripplot(x=\"hotel_cluster\", y=\"srch_destination_id\", data=expedia_train, jitter=True, edgecolor=\"gray\")\n",
    "\n",
    "# A violin plot combines the benefits of the previous two plots and simplifies them\n",
    "# Denser regions of the data are fatter, and sparser thiner in a violin plot\n",
    "sns.violinplot(x=\"hotel_cluster\", y=\"user_location_city\", data=expedia_train, size=10)\n",
    "\n",
    "# univariate relations = kdeplot,\n",
    "# which creates and visualizes a kernel density estimate of the underlying feature\n",
    "sns.FacetGrid(expedia_train, hue=\"hotel_cluster\", size=6) \\\n",
    "   .map(sns.kdeplot, \"user_location_city\") \\\n",
    "   .add_legend()\n",
    "\n",
    "\n",
    "# pairplot, which shows the bivariate relation between each pair of features\n",
    "# The diagonal elements in a pairplot show the histogram by default\n",
    "#sns.pairplot(expedia_train.drop(\"user_id\", axis=1), hue=\"hotel_cluster\", size=10)\n",
    "\n",
    "# pairplot - show other things, such as a kde\n",
    "#sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3, diag_kind=\"kde\")\n",
    "\n",
    "#  boxplot with Pandas on each feature split out by hotel cluster\n",
    "expedia_train.drop([\"user_id\", \"Unnamed: 0\"], axis=1).boxplot(by=\"hotel_cluster\", figsize=(12, 6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot\n",
    "average_children_by_region.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot - ggplot style\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "average_children_by_region.plot(kind = 'bar')\n",
    "average_children_by_region.plot(kind = 'bar', color = 'black', figsize=(15, 5))\n",
    "plt.title('Average # of Out-of-school Children of Primary School Age')\n",
    "plt.ylabel('# of Children')\n",
    "plt.xlabel('World Bank Regions')\n",
    "plt.xticks(rotation= 30)\n",
    "\n",
    "#group by year\n",
    "total_children_by_year = by_year['all_children'].sum()\n",
    "total_children_by_year.plot(kind = 'bar', color = 'black', figsize=(15, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#horizontal bar plot\n",
    "total_children_by_year.plot(kind = 'barh', color = 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram\n",
    "df_merged['percentage_female'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# boxplot\n",
    "pivot_female_percent_year_income.plot(kind = 'box', figsize=(15, 10))\n",
    "\n",
    "#overlay the actual data; this is generally most suitable with small- or moderate-sized data series.\n",
    "bp = titanic.boxplot(column='age', by='pclass', grid=False)\n",
    "for i in [1,2,3]:\n",
    "    y = titanic.age[titanic.pclass==i].dropna()\n",
    "    # Add some random \"jitter\" to the x-axis\n",
    "    x = np.random.normal(i, 0.04, size=len(y))\n",
    "    plt.plot(x, y, 'r.', alpha=0.2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "pivot_year_region = df_merged.pivot_table('all_children',\n",
    "                     index = 'year', columns = 'Region', aggfunc= 'mean')\n",
    "pivot_year_region.plot(figsize=(15, 10))\n",
    "\n",
    "#line plot-multi-varaibles\n",
    "\n",
    "variables = pd.DataFrame({'normal': np.random.normal(size=100), \n",
    "                       'gamma': np.random.gamma(1, size=100), \n",
    "                       'poisson': np.random.poisson(size=100)})\n",
    "variables.cumsum(0).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "total_female = df_merged[['all_children', 'percentage_female']]\n",
    "total_female.plot(kind ='scatter', x = 'all_children', y = 'percentage_female', figsize=(15, 10))\n",
    "#\n",
    "plt.scatter(baseball.ab, baseball.h, s=baseball.hr*10, alpha=0.5)\n",
    "plt.xlim(0, 700); plt.ylim(0, 200)\n",
    "\n",
    "plt.scatter(baseball.ab, baseball.h, c=baseball.hr, s=40, cmap='hot')\n",
    "plt.xlim(0, 700); plt.ylim(0, 200)\n",
    "\n",
    "#scatter matrix\n",
    "_ = pd.scatter_matrix(baseball.loc[:,'r':'sb'], figsize=(12,8), diagonal='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aj/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:2: FutureWarning: \n",
      "The rplot trellis plotting interface is deprecated and will be removed in a future version. We refer to external packages like seaborn for similar but more refined functionality. \n",
      "\n",
      "See our docs http://pandas.pydata.org/pandas-docs/stable/visualization.html#rplot for some example how to convert your existing code to these packages.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'titanic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-a3bc7f54318a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtitanic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m## graph for each pclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'titanic' is not defined"
     ]
    }
   ],
   "source": [
    "# Trellis plots\n",
    "from pandas.tools.rplot import *\n",
    "\n",
    "titanic = titanic[titanic.age.notnull() & titanic.fare.notnull()]\n",
    "## graph for each pclass\n",
    "tp = RPlot(titanic, x='age')\n",
    "tp.add(TrellisGrid(['pclass', 'sex']))\n",
    "tp.add(GeomDensity())\n",
    "_ = tp.render(plt.gcf())\n",
    "\n",
    "## graph for each week\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "bbp = RPlot(cdystonia, x='age', y='twstrs')\n",
    "bbp.add(TrellisGrid(['week', 'treat']))\n",
    "bbp.add(GeomScatter())\n",
    "bbp.add(GeomPolyFit(degree=2))\n",
    "_ = bbp.render(plt.gcf())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cdystonia' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dfe382cc9d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# polt 3 varaibles : 2 numerical, 1 categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcdystonia\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'site'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcdystonia\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRPlot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdystonia\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'twstrs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m cp.add(GeomPoint(colour=ScaleGradient('site', colour1=(1.0, 1.0, 0.5), colour2=(1.0, 0.0, 0.0)),\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cdystonia' is not defined"
     ]
    }
   ],
   "source": [
    "# polt 3 varaibles : 2 numerical, 1 categorical\n",
    "cdystonia['site'] = cdystonia.site.astype(float)\n",
    "plt.figure(figsize=(6,6))\n",
    "cp = RPlot(cdystonia, x='age', y='twstrs')\n",
    "cp.add(GeomPoint(colour=ScaleGradient('site', colour1=(1.0, 1.0, 0.5), colour2=(1.0, 0.0, 0.0)),\n",
    "            size=ScaleSize('week', min_size=10.0, max_size=200.0),\n",
    "            shape=ScaleShape('treat')))\n",
    "_ = cp.render(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-73fa5288fd31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtitanic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/titanic.xls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"titanic\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjitter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitanic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurvived\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "titanic = pd.read_excel(\"data/titanic.xls\", \"titanic\")\n",
    "titanic.name\n",
    "jitter = np.random.normal(scale=0.02, size=len(titanic))\n",
    "plt.scatter(np.log(titanic.fare), titanic.survived + jitter, alpha=0.3)\n",
    "plt.yticks([0,1])\n",
    "plt.ylabel(\"survived\")\n",
    "plt.xlabel(\"log(fare)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expedia_train[\"hotel_cluster\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expedia_train.plot(kind=\"scatter\", x=\"hotel_cluster\", y=\"site_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Viz - Seaboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can also use the seaborn library to make a similar plot\n",
    "# A seaborn jointplot shows bivariate scatterplots and univariate histograms in the same figure\n",
    "sns.jointplot(x=\"user_location_country\", y=\"hotel_market\", data=expedia_train, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One piece of information missing in the plots above is the cluster number\n",
    "# We'll use seaborn's FacetGrid to color the scatterplot by cluster number\n",
    "sns.FacetGrid(expedia_train, hue=\"hotel_cluster\", size=5) \\\n",
    "   .map(plt.scatter, \"channel\", \"hotel_market\") \\\n",
    "   .add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# individual feature in Seaborn through a boxplot\n",
    "sns.boxplot(x=\"hotel_cluster\", y=\"srch_destination_id\", data=expedia_train)\n",
    "# extend this plot is adding a layer of individual points on top of\n",
    "ax = sns.stripplot(x=\"hotel_cluster\", y=\"srch_destination_id\", data=expedia_train, jitter=True, edgecolor=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A violin plot combines the benefits of the previous two plots and simplifies them\n",
    "# Denser regions of the data are fatter, and sparser thiner in a violin plot\n",
    "sns.violinplot(x=\"hotel_cluster\", y=\"user_location_city\", data=expedia_train, size=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# univariate relations = kdeplot,\n",
    "# which creates and visualizes a kernel density estimate of the underlying feature\n",
    "sns.FacetGrid(expedia_train, hue=\"hotel_cluster\", size=6) \\\n",
    "   .map(sns.kdeplot, \"user_location_city\") \\\n",
    "   .add_legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pairplot, which shows the bivariate relation between each pair of features\n",
    "# The diagonal elements in a pairplot show the histogram by default\n",
    "#sns.pairplot(expedia_train.drop(\"user_id\", axis=1), hue=\"hotel_cluster\", size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pairplot - show other things, such as a kde\n",
    "#sns.pairplot(iris.drop(\"Id\", axis=1), hue=\"Species\", size=3, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  boxplot with Pandas on each feature split out by hotel cluster\n",
    "expedia_train.drop([\"user_id\", \"Unnamed: 0\"], axis=1).boxplot(by=\"hotel_cluster\", figsize=(12, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Andrews Curves /pandas - using attributes of samples as coefficients for Fourier series and then plotting these\n",
    "#from pandas.tools.plotting import andrews_curves\n",
    "#andrews_curves(expedia_train.drop([\"user_id\", \"Unnamed: 0\"], axis=1), \"hotel_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parallel coordinates plots each feature on a separate column & then draws lines\n",
    "#from pandas.tools.plotting import parallel_coordinates\n",
    "#parallel_coordinates(expedia_train.drop([\"user_id\", \"Unnamed: 0\"], axis=1), \"hotel_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multivariate visualization/ pandas/radviz - puts each feature as a point on a 2D plane, and then simulates\n",
    "# having each sample attached to those points through a spring weighted by the relative value for that feature\n",
    "#from pandas.tools.plotting import radviz\n",
    "#radviz(expedia_train.drop(\"user_id\", axis=1), \"hotel_cluster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Data Modeling - Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fitting data to probability distributions\n",
    "#view\n",
    "_ = precip.hist(sharex=True, sharey=True, grid=False)\n",
    "plt.tight_layout()\n",
    "\n",
    "#gamma distribution\n",
    "The data are skewed, with a longer tail to the right than to the left\n",
    "The data are positive-valued, since they are measuring rainfall\n",
    "The data are continuous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-52-a1eea1d24392>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-a1eea1d24392>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <img src=\"Gamma_distribution.png\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"Gamma_distribution.png\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.distributions import gamma\n",
    "\n",
    "precip.Jan.hist(normed=True, bins=20)\n",
    "plt.plot(np.linspace(0, 10), gamma.pdf(np.linspace(0, 10), alpha_mom[0], beta_mom[0]))\n",
    "#Looping over all months, we can create a grid of plots for the distribution of rainfall, using the gamma distribution:\n",
    "\n",
    "\n",
    "axs = precip.hist(normed=True, figsize=(12, 8), sharex=True, sharey=True, bins=15, grid=False)\n",
    "\n",
    "for ax in axs.ravel():\n",
    "    \n",
    "    # Get month\n",
    "    m = ax.get_title()\n",
    "    \n",
    "    # Plot fitted distribution\n",
    "    x = np.linspace(*ax.get_xlim())\n",
    "    ax.plot(x, gamma.pdf(x, alpha_mom[m], beta_mom[m]))\n",
    "    \n",
    "    # Annotate with parameter estimates\n",
    "    label = 'alpha = {0:.2f}\\nbeta = {1:.2f}'.format(alpha_mom[m], beta_mom[m])\n",
    "    ax.annotate(label, xy=(10, 0.2))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Kernel density estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ExtraTreesClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c22afd0a9d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ExtraTreesClassifier'"
     ]
    }
   ],
   "source": [
    "# Feature-engineering \n",
    "\n",
    "#combining variables with the same unit and binning them\n",
    "\n",
    "import sklearn \n",
    "\n",
    "from sklearn.tree import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA --> then do Deep neural networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep neural networks --> Dato/remove the classification layer -->/OR assign labels \n",
    "@data.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##data Classifier\n",
    "http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/classification.html\n",
    "http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/dataClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## # Feature selection   @@ Python @@\n",
    "clf = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf.fit(X_train, y_train)\n",
    "# clf.feature_importances_ \n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_test = fs.transform(X_test)\n",
    "test = fs.transform(test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sci' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4acccae0d677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSci\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Sci' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Filtering \n",
    "Perhaps the simplest  ltering scheme is to evaluate each feature individually based on its correlation with the target function  e g   using a mutual information measure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tree selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#attribute weighting method-perceptron updating rule / The Winnow algorithm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sci' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cf86b6762f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSci\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mkit\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#1.13. Feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#1.13.1. Removing features with low variance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#1.13.2. Univariate feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#1.13.3. Recursive feature elimination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sci' is not defined"
     ]
    }
   ],
   "source": [
    "Sci-kit-learn\n",
    "#1.13. Feature selection\n",
    "#1.13.1. Removing features with low variance\n",
    "#1.13.2. Univariate feature selection\n",
    "#1.13.3. Recursive feature elimination\n",
    "#1.13.4. Feature selection using SelectFromModel\n",
    "#1.13.4.1. L1-based feature selection\n",
    "#1.13.4.2. Randomized sparse models\n",
    "#1.13.4.3. Tree-based feature selection\n",
    "#1.13.5. Feature selection as part of a pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.neural_network\n",
    "    \n",
    "preprocessing.Binarizer([threshold, copy])\tBinarize data (set feature values to 0 or 1) according to a threshold\n",
    "preprocessing.FunctionTransformer([func, ...])\tConstructs a transformer from an arbitrary callable.\n",
    "preprocessing.Imputer([missing_values, ...])\tImputation transformer for completing missing values.\n",
    "preprocessing.KernelCenterer\tCenter a kernel matrix\n",
    "preprocessing.LabelBinarizer([neg_label, ...])\tBinarize labels in a one-vs-all fashion\n",
    "preprocessing.LabelEncoder\tEncode labels with value between 0 and n_classes-1.\n",
    "preprocessing.MultiLabelBinarizer([classes, ...])\tTransform between iterable of iterables and a multilabel format\n",
    "preprocessing.MaxAbsScaler([copy])\tScale each feature by its maximum absolute value.\n",
    "preprocessing.MinMaxScaler([feature_range, copy])\tTransforms features by scaling each feature to a given range.\n",
    "preprocessing.Normalizer([norm, copy])\tNormalize samples individually to unit norm.\n",
    "preprocessing.OneHotEncoder([n_values, ...])\tEncode categorical integer features using a one-hot aka one-of-K scheme.\n",
    "preprocessing.PolynomialFeatures([degree, ...])\tGenerate polynomial and interaction features.\n",
    "preprocessing.RobustScaler([with_centering, ...])\tScale features using statistics that are robust to outliers.\n",
    "preprocessing.StandardScaler([copy, ...])\tStandardize features by removing the mean and scaling to unit variance\n",
    "preprocessing.add_dummy_feature(X[, value])\tAugment dataset with an additional dummy feature.\n",
    "preprocessing.binarize(X[, threshold, copy])\tBoolean thresholding of array-like or scipy.sparse matrix\n",
    "preprocessing.label_binarize(y, classes[, ...])\tBinarize labels in a one-vs-all fashion\n",
    "preprocessing.maxabs_scale(X[, axis, copy])\tScale each feature to the [-1, 1] range without breaking the sparsity.\n",
    "preprocessing.minmax_scale(X[, ...])\tTransforms features by scaling each feature to a given range.\n",
    "preprocessing.normalize(X[, norm, axis, copy])\tScale input vectors individually to unit norm (vector length).\n",
    "preprocessing.robust_scale(X[, axis, ...])\tStandardize a dataset along any axis\n",
    "preprocessing.scale(X[, axis, with_mean, ...])\tStandardize a dataset along any axis\n",
    "\n",
    "\n",
    "feature_selection.GenericUnivariateSelect([...])\tUnivariate feature selector with configurable strategy.\n",
    "feature_selection.SelectPercentile([...])\tSelect features according to a percentile of the highest scores.\n",
    "feature_selection.SelectKBest([score_func, k])\tSelect features according to the k highest scores.\n",
    "feature_selection.SelectFpr([score_func, alpha])\tFilter: Select the pvalues below alpha based on a FPR test.\n",
    "feature_selection.SelectFdr([score_func, alpha])\tFilter: Select the p-values for an estimated false discovery rate\n",
    "feature_selection.SelectFromModel(estimator)\tMeta-transformer for selecting features based on importance weights.\n",
    "feature_selection.SelectFwe([score_func, alpha])\tFilter: Select the p-values corresponding to Family-wise error rate\n",
    "feature_selection.RFE(estimator[, ...])\tFeature ranking with recursive feature elimination.\n",
    "feature_selection.RFECV(estimator[, step, ...])\tFeature ranking with recursive feature elimination and cross-validated selection of the best number of features.\n",
    "feature_selection.VarianceThreshold([threshold])\tFeature selector that removes all low-variance features.\n",
    "feature_selection.chi2(X, y)\tCompute chi-squared stats between each non-negative feature and class.\n",
    "feature_selection.f_classif(X, y)\tCompute the ANOVA F-value for the provided sample.\n",
    "feature_selection.f_regression(X, y[, center])\tUnivariate linear regression tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Spark data feature engineering:\n",
    "    binarizer\n",
    "    bucketizer\n",
    "    chi-squared selection\n",
    "    CountVectorizer\n",
    "    Discrete cosine transform\n",
    "    ElementwiseProduct\n",
    "    Hashing term frequency\n",
    "    MinMaxScaler\n",
    "    Ngram\n",
    "    Normalizer\n",
    "    One-hot Encoder\n",
    "    PCA\n",
    "    Polynomial Expansion\n",
    "    RFormula\n",
    "    SQLTransformer\n",
    "    Standard scaler\n",
    "    StopWordsRemover\n",
    "    StringIndexer\n",
    "    VectorAssembler\n",
    "    VectorIndexer\n",
    "    VectorSlicer\n",
    "    Word2vec\n",
    "    TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Text similarities between search term and product title/description/attributes\n",
    "\n",
    "    BOW Cosine Similairty\n",
    "    TF-IDF Cosine Similarity\n",
    "    Jaccard Similarity\n",
    "    *Edit Distance\n",
    "    Word2Vec Distance (I didn’t include this because of poor performance. Yet it seems that I was using it wrong.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the Numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-b4dfa32f47ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#?? (maybe not a categorical)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# perform z-score scaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeatures_mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfeatures_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mstd_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfeatures_mu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfeatures_sigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# normalize \n",
    "#?? (maybe not a categorical)\n",
    "# perform z-score scaling\n",
    "features_mu = np.mean(features, axis=0)\n",
    "features_sigma = np.std(features, axis=0)\n",
    "std_features = (features - features_mu)/features_sigma\n",
    "\n",
    "# perform standard scaling again but via SciKit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "z_scaler = StandardScaler()\n",
    "movie_features = z_scaler.fit_transform(movie_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Reorder- NumPy's permutation function-create an index\n",
    "new_order = np.random.permutation(len(segments))\n",
    "new_order[:30]\n",
    "\n",
    "segments.take(new_order).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## unsupervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Latent Class Analysis --> smaller groups\n",
    "#eg. # of logins, avg course completion, #avg test score, average session duration, number of courses taken ==> high achievers, mispaced , dropouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nba_shot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b70279974af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# split data into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.80\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnba_shot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnba_shot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_set_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_Coordinate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y_Coordinate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnba_shot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_Coordinate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y_Coordinate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nba_shot_data' is not defined"
     ]
    }
   ],
   "source": [
    "#split\n",
    "# split data into train and test\n",
    "train_set_size = int(.80*len(nba_shot_data))\n",
    "train_features = nba_shot_data.ix[:train_set_size,['x_Coordinate','y_Coordinate']].as_matrix()\n",
    "test_features = nba_shot_data.ix[train_set_size:,['x_Coordinate','y_Coordinate']].as_matrix()\n",
    "train_class_labels = nba_shot_data.ix[:train_set_size,['shot_outcome']].as_matrix()\n",
    "test_class_labels = nba_shot_data.ix[train_set_size:,['shot_outcome']].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform features so they are more representative  i.e. x,y to radians and radius\n",
    "\n",
    "\n",
    "#from cosine to linear \n",
    "z=np.sin(x+np.pi/2)\n",
    "\n",
    "## temperation during the day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# passing in the label to the algorihtm\n",
    "np.ravel(train_class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "methods take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target: A one-dimensional numpy array containing the target/response from the train data.\n",
    "target = train[\"Survived\"].values\n",
    "labels = (movie_data['IMDB'] >= 7.).astype('int').tolist()\n",
    "\n",
    "# features: A multidimensional numpy array containing the features/predictors from the train data.\n",
    "features = train[[\"Sex\", \"Age\"]].values\n",
    "features = movie_data[['RottenTomatoes','RottenTomatoes_User','Metacritic','Metacritic_User','Fandango_Ratingvalue']].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### randomize (remove bais)\n",
    "\n",
    "?? np.random.shuffle(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4bda4aded1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrange'"
     ]
    }
   ],
   "source": [
    "#### function for shuffling the data and labels\n",
    "def shuffle_in_unison(features, labels):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(features)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "## start_time = time.time()\n",
    "##model\n",
    "end_time = time.time()\n",
    "#print \"Training ended after %.2f seconds.\" %(end_time-start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Planning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tools\n",
    "# python\n",
    "# R\n",
    "# Orange toolkit http://orange.biolab.si\n",
    "# H20\n",
    "# Spark\n",
    "# WEKA/WEKA\n",
    "# Wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preventing Overfitting/Underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overfitting - usually with many features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-56-0faa23da9ead>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-56-0faa23da9ead>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    l1/LASSO - encourage the sum of the absoulte values of the parameters to be small..>>couses some parameters to be zero\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# LASSO/Linear least squares \n",
    "l1/LASSO - encourage the sum of the absoulte values of the parameters to be small..>>couses some parameters to be zero\n",
    "l2- encourage sum of teh squares of the parameters to be small\n",
    "(rotationally invariant algorithm - worst case sample complexity that grows with linearly with irrelevant features)\n",
    "=SVM, multi-layer NN, perceptron, unregularized Logit regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling - Bootstrapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.permutation(titanic.name)[:5]\n",
    "random_ind = np.random.randint(0, len(titanic), 5)\n",
    "titanic.name[random_ind]\n",
    "n = 10\n",
    "R = 1000\n",
    "# Original sample (n=10)\n",
    "x = np.random.normal(size=n)\n",
    "# 1000 bootstrap samples of size 10\n",
    "s = [x[np.random.randint(0,n,n)].mean() for i in range(R)]\n",
    "_ = plt.hist(s, bins=30)\n",
    "#Bootstrap Estimates¶\n",
    "boot_mean = np.sum(s)/R\n",
    "boot_mean\n",
    "boot_var = ((np.array(s) - boot_mean) ** 2).sum() / (R-1)\n",
    "boot_var\n",
    "https://github.com/fonnesbeck/statistical-analysis-python-tutorial/blob/master/4.%20Statistical%20Data%20Modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-59-aeb98fb955be>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-59-aeb98fb955be>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    There are two sources of error in bootstrap estimates:\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Bootstrap error\n",
    "There are two sources of error in bootstrap estimates:\n",
    "Sampling error from the selection of S.\n",
    "Bootstrap error from failing to enumerate all possible bootstrap samples.\n",
    "For the sake of accuracy, it is prudent to choose at least R=1000\n",
    "https://github.com/fonnesbeck/statistical-analysis-python-tutorial/blob/master/4.%20Statistical%20Data%20Modeling.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-a96e575b8225>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-a96e575b8225>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    http://scikit-learn.org/stable/modules/model_evaluation.html\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "‘accuracy’\tmetrics.accuracy_score\t \n",
    "‘average_precision’\tmetrics.average_precision_score\t \n",
    "‘f1’\tmetrics.f1_score\tfor binary targets\n",
    "‘f1_micro’\tmetrics.f1_score\tmicro-averaged\n",
    "‘f1_macro’\tmetrics.f1_score\tmacro-averaged\n",
    "‘f1_weighted’\tmetrics.f1_score\tweighted average\n",
    "‘f1_samples’\tmetrics.f1_score\tby multilabel sample\n",
    "‘log_loss’\tmetrics.log_loss\trequires predict_proba support\n",
    "‘precision’ etc.\tmetrics.precision_score\tsuffixes apply as with ‘f1’\n",
    "‘recall’ etc.\tmetrics.recall_score\tsuffixes apply as with ‘f1’\n",
    "‘roc_auc’\tmetrics.roc_auc_score\t \n",
    "##Clustering\t \t \n",
    "‘adjusted_rand_score’\tmetrics.adjusted_rand_score\t \n",
    "##Regression\t \t \n",
    "‘mean_absolute_error’\tmetrics.mean_absolute_error\t \n",
    "‘mean_squared_error’\tmetrics.mean_squared_error\t \n",
    "‘median_absolute_error’\tmetrics.median_absolute_error\t \n",
    "‘r2’\tmetrics.r2_score\n",
    "\n",
    "#Classification\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "##Some of these are restricted to the binary classification case:\n",
    "matthews_corrcoef(y_true, y_pred)\tCompute the Matthews correlation coefficient (MCC) for binary classes\n",
    "precision_recall_curve(y_true, probas_pred)\tCompute precision-recall pairs for different probability thresholds\n",
    "roc_curve(y_true, y_score[, pos_label, ...])\tCompute Receiver operating characteristic (ROC)\n",
    "\n",
    "##Others also work in the multiclass case:\n",
    "confusion_matrix(y_true, y_pred[, labels])\tCompute confusion matrix to evaluate the accuracy of a classification\n",
    "hinge_loss(y_true, pred_decision[, labels, ...])\tAverage hinge loss (non-regularized)\n",
    "##Some also work in the multilabel case:\n",
    "accuracy_score(y_true, y_pred[, normalize, ...])\tAccuracy classification score.\n",
    "classification_report(y_true, y_pred[, ...])\tBuild a text report showing the main classification metrics\n",
    "f1_score(y_true, y_pred[, labels, ...])\tCompute the F1 score, also known as balanced F-score or F-measure\n",
    "fbeta_score(y_true, y_pred, beta[, labels, ...])\tCompute the F-beta score\n",
    "hamming_loss(y_true, y_pred[, classes])\tCompute the average Hamming loss.\n",
    "jaccard_similarity_score(y_true, y_pred[, ...])\tJaccard similarity coefficient score\n",
    "log_loss(y_true, y_pred[, eps, normalize, ...])\tLog loss, aka logistic loss or cross-entropy loss.\n",
    "precision_recall_fscore_support(y_true, y_pred)\tCompute precision, recall, F-measure and support for each class\n",
    "precision_score(y_true, y_pred[, labels, ...])\tCompute the precision\n",
    "recall_score(y_true, y_pred[, labels, ...])\tCompute the recall\n",
    "zero_one_loss(y_true, y_pred[, normalize, ...])\tZero-one classification loss.\n",
    "##And some work with binary and multilabel (but not multiclass) problems:\n",
    "average_precision_score(y_true, y_score[, ...])\tCompute average precision (AP) from prediction scores\n",
    "roc_auc_score(y_true, y_score[, average, ...])\tCompute Area Under the Curve (AUC) from prediction scores\n",
    "\n",
    "#Multilabel ranking metrics\n",
    "\n",
    "#Regression metrics\n",
    "\n",
    "#clustering metrics \n",
    "\n",
    "#dummy estimators\n",
    "\n",
    "\n",
    "### calculate classification errors\n",
    "# return a percentage: (number misclassified)/(total number of datapoints)\n",
    "def calc_classification_error(predictions, class_labels):\n",
    "    n = predictions.size\n",
    "    num_of_errors = 0.\n",
    "    for idx in xrange(n):\n",
    "        if (predictions[idx] >= 0.5 and class_labels[idx]==0) or (predictions[idx] < 0.5 and class_labels[idx]==1):\n",
    "            num_of_errors += 1\n",
    "    return num_of_errors/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FukuML simple machine learning library"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://pypi.python.org/pypi/FukuML/0.1.4\n",
    "# Perceptron\n",
    "Perceptron Binary Classification Learning Algorithm\n",
    "Perceptron Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Perceptron Multi Classification Learning Algorithm\n",
    "Perceptron Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Pocket Perceptron Binary Classification Learning Algorithm\n",
    "Pocket Perceptron Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Pocket Perceptron Multi Classification Learning Algorithm\n",
    "Pocket Perceptron Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "\n",
    "# Regression\n",
    "Linear Regression Learning Algorithm\n",
    "Linear Regression Binary Classification Learning Algorithm\n",
    "Linear Regression Multi Classification Learning Algorithm\n",
    "Logistic Regression Learning Algorithm\n",
    "Logistic Regression Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression Binary Classification Learning Algorithm\n",
    "Logistic Regression Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression One vs All Multi Classification Learning Algorithm\n",
    "Logistic Regression One vs All Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression One vs One Multi Classification Learning Algorithm\n",
    "Logistic Regression One vs One Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "\n",
    "# Classification\n",
    "https://www.datarobot.com/blog/classification-with-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-7eb0e22c487e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7eb0e22c487e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    naiveBayes.py\t= naive Bayes classifier. http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/naiveBayes.html\u001b[0m\n\u001b[0m                 \t            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "naiveBayes.py\t= naive Bayes classifier. http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/naiveBayes.html\n",
    "perceptron.py\t= perceptron classifier.  http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/perceptron.html\n",
    "mira.py\tMIRA classifier.   = http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/mira.html\n",
    "Nilearn = http://nilearn.github.io/decoding/estimator_choice.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  https://github.com/dmlc/xgboost/tree/master/demo/multiclass_classification (Multi-class example X-GBoost)\n",
    "# Scikit-multi learn : http://scikit.ml\n",
    "# One versus All:\tsklearn.multiclass.OneVsRestClassifier\n",
    "# Multi-class and multi-label --http://scikit-learn.org/stable/modules/multiclass.html \n",
    "#http://scikit-learn.org/stable/modules/ensemble.html\n",
    "# http://scikit-learn.org/stable/modules/ensemble.html \n",
    "# http://www.codeproject.com/Articles/821347/MultiClass-Logistic-Classifier-in-Python (implementation in python)\n",
    "# multi-label http://scikit-learn.org/stable/auto_examples/plot_multilabel.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train and predict\n",
    "# Import the linearregression model.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize the model class.\n",
    "model = LinearRegression()\n",
    "# Fit the model to the training data.\n",
    "model.fit(train[columns], train[target])\n",
    "\n",
    "# Generate our predictions for the test set.\n",
    "predictions = model.predict(test[columns])\n",
    "\n",
    "\n",
    "# model options\n",
    "\n",
    "# test\n",
    "\n",
    "# Import the scikit-learn function to compute error.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Compute error between our test predictions and the actual values.\n",
    "mean_squared_error(predictions, test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set the random number generator for reproducability\n",
    "np.random.seed(123)\n",
    "\n",
    "# let's try to predict the IMDB rating from the others\n",
    "features = movie_data[['RottenTomatoes','RottenTomatoes_User','Metacritic','Metacritic_User','Fandango_Ratingvalue']].as_matrix()\n",
    "# create classes: more or less that 7/10 rating\n",
    "labels = (movie_data['IMDB'] >= 7.).astype('int').tolist()\n",
    "shuffle_in_unison(features, labels)\n",
    "\n",
    "### Your Code Goes Here ###\n",
    "\n",
    "# initialize and train a logistic regression model\n",
    "lm = LogisticRegression()\n",
    "lm.fit(features, labels)\n",
    "\n",
    "# compute error on training data\n",
    "predictions = lm.predict(features)\n",
    "train_error_rate = calc_classification_error(predictions, labels)\n",
    "\n",
    "###########################\n",
    "\n",
    "print \"Classification error on training set: %.2f%%\" %(train_error_rate*100)\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %((sum(labels)*100.)/len(labels))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f516a396281d>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f516a396281d>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    .feature_importances_ #importance: requesting the\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#decision trees -- Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "#training\n",
    "\n",
    "my_tree = tree.DecisionTreeClassifier()\n",
    "my_tree = my_tree.fit(features, target)\n",
    "\n",
    "#analysis & testing:\n",
    "\n",
    ".feature_importances_ #importance: requesting the \n",
    ".score() # mean accuracy that you can compute using the\n",
    "\n",
    "    print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))\n",
    "clf.score(features_test,labels_test)\n",
    "\n",
    "#control overfitting\n",
    "# DecisionTreeRegressor\n",
    "#, the depth of our model is defined by two parameters: \n",
    "#    - the max_depth parameter determines when the splitting up of the decision tree stops. \n",
    "#    - the min_samples_split parameter monitors the amount of observations in a bucket. \n",
    "#If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.\n",
    "#By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!\n",
    "\n",
    "# Create a new array with the added features: features_two\n",
    "features_two = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n",
    "\n",
    "\n",
    "#2-D plotting\n",
    "%pylab inline\n",
    "import sys #?\n",
    "import matplotlib.pyplot as plt #?\n",
    "import numpy as np #?\n",
    "import pylab as pl #?\n",
    "sys.path.append('../tools') #?\n",
    "Populating the interactive namespace from numpy and matplotlib #?\n",
    "from class_vis import prettyPicture, output_image\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "# output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "## # Feature selection   @@ Python @@\n",
    "clf = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf.fit(X_train, y_train)\n",
    "# clf.feature_importances_ \n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_test = fs.transform(X_test)\n",
    "test = fs.transform(test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest \n",
    "#handles the overfitting problem you faced with decision trees. \n",
    "#grows multiple (very deep) classification trees using the training set.\n",
    "# each tree is used to come up with a prediction and every outcome is counted as a vote\n",
    "# majority's vote count as the actual classification decision, avoids overfitting.\n",
    "\n",
    "#n_estimators=allows you to set the number of trees you wish to plant and average over.\n",
    "\n",
    "\n",
    "#Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables\n",
    "features_forest = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "#Building the Forest: my_forest\n",
    "n_estimators = 100\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators=100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "#Print the score of the random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "#Compute predictions and print the length of the prediction vector:test_features, pred_forest\n",
    "test_features = test[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "pred_forest = my_forest.predict(test_features)\n",
    "print(len(pred_forest))\n",
    "\n",
    "\n",
    "#feature importances and score\n",
    "\n",
    "#Request and print the `.feature_importances_` attribute\n",
    "print(my_tree_two.feature_importances_)\n",
    "print(my_forest.feature_importances_)\n",
    "\n",
    "#Compute and print the mean accuracy score for both models\n",
    "print(my_tree_two.score(features_two, target))\n",
    "print(my_forest.score(features_two, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-e5a6372a47b0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-e5a6372a47b0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    http://napitupulu-jon.appspot.com/posts/decision-tree-ud.html\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test mining\n",
    "http://napitupulu-jon.appspot.com/posts/decision-tree-ud.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Deep Learning - Alex net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import graphlab as gl\n",
    "import networkx as nx\n",
    "from matplotlib.pyplot import *\n",
    "#gl.canvas.set_target('ipynb')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python-Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Numpy-Linear Algebra/Arrays\n",
    "\n",
    "http://scipy.github.io/old-wiki/pages/Tentative_NumPy_Tutorial.html#A.22Automatic.22_Reshaping \n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# you can create a 1-d array with a list of numbers\n",
    "a = np.array([1, 4, 6])\n",
    "print np.ones((3, 4))\n",
    "print np.zeros((2, 5))\n",
    "\n",
    "#Transpose matrix\n",
    "print b.T\n",
    "\n",
    "#Inverse a matrix\n",
    "m_inverse = np.linalg.inv(m)\n",
    "\n",
    "# you can convert a 1-d array to a 2-d array with np.newaxis\n",
    "(3,)-> a[np.newaxis].shape: (1, 3)\n",
    "\n",
    "#reshape\n",
    "\n",
    "#concate arrays\n",
    "np.hstack([b, b]):\n",
    "np.vstack([b, b]):\n",
    "\n",
    "#matrix multiplication\n",
    "# you can perform matrix multiplication with np.dot()\n",
    "c = np.dot(a, b)\n",
    "\n",
    "# you can perform element-wise multiplication with * \n",
    "d = b * b\n",
    "\n",
    "#decision stuctures\n",
    "\n",
    "for this_row in b:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pandas-Scientific Computing/Data Frames\n",
    "http://pandas-docs.github.io/pandas-docs-travis/ \n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data=b,  columns=['Weight', 'Height'])\n",
    "\n",
    "\n",
    "# read .csv files\n",
    "baseball = pd.read_csv('data/baseball.dat.txt')\n",
    "\n",
    "# save .csv files\n",
    "\n",
    "\n",
    "## expore data\n",
    "\n",
    "baseball.head()  \t#first five rows\n",
    "baseball.keys()  \t\t# column name, and data\n",
    "baseball.info()\t\t\t# column data type, and no. of rows \n",
    "Salary                     337 non-null int64\n",
    "baseball.describe()\t\t#summary of mean, sd, count and percentiles\n",
    "\n",
    "# Query data\n",
    "millionaire_indices = baseball['Salary'] > 1000\t\t\t\t\t   # filter\n",
    "print (\"baseball[millionaire_indices].shape:\", baseball[millionaire_indices].shape)    # by column\n",
    "baseball[millionaire_indices][['Salary', 'AVG', 'Runs', 'Name']]\t                   #subset by column\n",
    "\n",
    "\n",
    "#joins\n",
    "merged = pd.merge(baseball, shoe_size_df, on=['Name'])\n",
    "merged_outer = pd.merge(baseball, shoe_sizes, on=['Name'], how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "decision-tree-ud_files/4.jpg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Presentation\n",
    "\n",
    "#display image from pc\n",
    "from IPython.display import Image\n",
    "Image('decision-tree-ud_files/4.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Other Datasets\n",
    "- [World Bank Data](http://data.worldbank.org/)\n",
    "- [United Nations Data](http://data.un.org/)\n",
    "- [Toronto Open Data](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=1a66e03bb8d1e310VgnVCM10000071d60f89RCRD)\n",
    "- [Reddit /r/datasets](reddit.com/r/datasets)\n",
    "- [100+ Interesting Datasets](http://rs.io/100-interesting-data-sets-for-statistics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
