{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Code library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# more on the web\n",
    "http://blog.yhat.com/posts/data-science-in-python-tutorial.html\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read from CSV\n",
    "train=read.csv(\"train.csv\")\n",
    "test=read.csv(\"santander_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Download data files\n",
    "!wget --no-check-certificate --output-document countries_metadata.csv https://ibm.box.com/shared/static/qh3o86mpij17ot7anydcmbwt41lwxvln.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read & write feature files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read csv using pandas\n",
    "import pandas as pd\n",
    "\n",
    "df_outofschool = pd.read_csv('outofschoolchildren.csv', skiprows = 2)\n",
    "train_url = \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\n",
    "train = pd.read_csv(train_url)\n",
    "expedia_pred3 = pd.read_csv('preds3rf.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read form zip\n",
    "# un-zip the paintings file\n",
    "import zipfile\n",
    "zipper = zipfile.ZipFile('./data/bob_ross/bob_ross_paintings.npy.zip')\n",
    "zipper.extractall('./data/bob_ross/')\n",
    "\n",
    "# load the 403 x 360,000 matrix\n",
    "br_paintings = np.load(open('./data/bob_ross/bob_ross_paintings.npy','rb'))\n",
    "print \"Dataset size: %d x %d\"%(br_paintings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Compile and run as as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Numba \n",
    "@python\n",
    "numba.__version__\n",
    "'0.25.0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## profile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train.head())\n",
    "df_outofschool.tail(10)\n",
    ".describe() #explore a DataFrame using the \n",
    ".shape attribute of your DataFrame object. (ex. your_data.shape)\n",
    "df_outofschool.columns\n",
    "df_merged.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deeper profile\n",
    "len(df_outofschool.country_code.unique())\n",
    "\n",
    "#unique - columns with \"\" in the names\n",
    "#but how to access columns with \"\" in their names?\n",
    "#iloc\n",
    "len(df_merged.iloc[:,-5].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deeper exploration\n",
    "#factors\n",
    "print(train[\"Survived\"].value_counts())  #counts by factor\n",
    "print(train[\"Survived\"].value_counts(normalize = True))   # proportions\n",
    "\n",
    "#factors-subet\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts()) # Females that survived vs Females that passed away\n",
    "print(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True))  # proportion and subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create/add new columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create new variables\n",
    "your_data[\"new_var\"] = 10   # 10 for each observation\n",
    " #(i) create a new column, and \n",
    "    #(ii) provide the values for each observation (i.e., row) based on the age of the passenger.\n",
    "#example    \n",
    "train[\"Child\"] = float('NaN')\n",
    "train.Child[train.Age < 18] = 1\n",
    "train.Child[train.Age >= 18] = 0\n",
    "print(train.Child)\n",
    "\n",
    "\n",
    "# Initialize a Survived column to 0\n",
    "test_one[\"Survived\"] = 0\n",
    "# Set Survived to 1 if Sex equals \"female\" and print the `Survived` column from `test_one`\n",
    "test_one[\"Survived\"][train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "\n",
    "#df_merged['percentage_male'] = 100- df_merged['percentage_female']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#rename column headers\n",
    "df_outofschool.rename(columns={'Unnamed: 1':'year'}, inplace= True)\n",
    "#change all columns\n",
    "df_outofschool.columns = ['country', 'year', 'empty', 'all_children', 'female', 'male', 'percentage_female']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shift up first column\n",
    "df_outofschool.country = df_outofschool.country.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e0c8ca5b0556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#drop a column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'empty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "#drop a column\n",
    "df_outofschool.drop('empty', axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SORT\n",
    "average_children_by_region.sort(ascending = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-408438c74822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#remove rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#removing bottom rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3893\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#alternative methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#df_outofschool.drop(df_outofschool.tail(7).index)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "#remove rows \n",
    "#removing bottom rows\n",
    "df_outofschool.drop(df_outofschool.index[3893:3900], inplace= True)\n",
    "#alternative methods\n",
    "#df_outofschool.drop(df_outofschool.tail(7).index)\n",
    "#df_outofschool.drop(list(range(3894,3900)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fill names down to the next (replace NaN below wiht country names)\n",
    "df_outofschool['country'] = df_outofschool['country'].ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace cells with ..\n",
    "#fill the '..' cells with NaN\n",
    "df_outofschool.replace('..', np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd2912f4ed0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#cleaning/imputing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#Impute the Embarked variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Embarked\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#cleaning/imputing\n",
    "train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())\n",
    "#Impute the Embarked variable\n",
    "train[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "test.Fare[152] = test.Fare.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_outofschool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5bcbfc9e3aef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# country=AFG: Afghanistan      # -> country_code=AFG   country=Afganistan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#first look at the string values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_outofschool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_outofschool' is not defined"
     ]
    }
   ],
   "source": [
    "# split text into two columns\n",
    "# country=AFG: Afghanistan\t# -> country_code=AFG   country=Afganistan\n",
    "#first look at the string values\n",
    "df_outofschool.loc[0, 'country']\n",
    "df_outofschool.loc[0, 'country'].split(': ')\n",
    "df_outofschool.loc[0, 'country'].split(': ')[0]\n",
    "get_country_code = lambda string: string.split(': ')[0]\n",
    "#to look at what it does\n",
    "df_outofschool['country'].apply(get_country_code)\n",
    "df_outofschool['country_code'] =  df_outofschool['country'].apply(get_country_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove rows with no data\n",
    "df_merged.dropna(axis = 0, how = 'all', subset= df_merged.columns[2:6], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a762f2504acf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# change data type -float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#selct by column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_merged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# change data type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "# change data type -float\n",
    "df_merged[df_merged.columns[2:6]].astype(float)   #selct by column name\n",
    "df_merged[df_merged.columns[2:6]] = df_merged[df_merged.columns[2:6]].astype(float)\n",
    "# change data type \n",
    "-int\n",
    "df_merged[df_merged.columns[1]] = df_merged[df_merged.columns[1]].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rounding off decimal places\n",
    "np.round(pivot_year_region, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation/Munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transformation\n",
    "#Convert the male and female groups to integer form\n",
    "train[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\n",
    "train[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n",
    "\n",
    "\n",
    "#Convert the Embarked classes to integer form\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\n",
    "train[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2\n",
    "\n",
    "#Print the Sex and Embarked columns\n",
    "print(train[\"Sex\"])\n",
    "print(train[\"Embarked\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### enriching through joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making sure column join names match 1:1\n",
    "df_metadata.rename(columns={'Country Code' : 'country_code'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'http://i.stack.imgur.com/sKS1u.jpg', width = 800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_outofschool, df_metadata, on= 'country_code', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#add column to groupby\n",
    "by_country = df_merged.groupby('country')\n",
    "In [82]:\n",
    "\n",
    "type(by_country)\n",
    "#Out[82]:\n",
    "#pandas.core.groupby.DataFrameGroupBy\n",
    "\n",
    "# counts shown by the following group by data structure\n",
    "by_country.count()\n",
    "\n",
    "#show counts by selected columns\n",
    "by_country.count().iloc[:,1:5]\n",
    "\n",
    "\"\"\n",
    "by_year = df_merged.groupby('year')\n",
    "df_merged.dtypes\n",
    "#Out[105]\n",
    "country               object\n",
    "year                  object\n",
    "all_children          object\n",
    "\"\"\n",
    "df_merged.columns[2:6] # column names\n",
    "\n",
    "by_region['all_children'].sum()\n",
    "by_region['all_children'].mean()\n",
    "by_region['all_children'].max()\n",
    "#Region\n",
    "#East Asia & Pacific            1557369.0\n",
    "#Europe & Central Asia           397419.0\n",
    "#Latin America & Caribbean       801014.0\n",
    "\n",
    "#Group by Region then by year, value = mean of all_children\n",
    "df_merged.groupby(['Region', 'year'])['all_children'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossTab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-3e68d5fbe881>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_merged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Region'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all_children'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "df_merged.groupby(['Region', 'year'])['all_children'].mean().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Pivot table\n",
    "df_merged.pivot_table('all_children',\n",
    "                     index = 'year', columns = 'Region', aggfunc= 'mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_merged[df_merged['all_children'] == 1338154]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration/Vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot\n",
    "average_children_by_region.plot(kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot - ggplot style\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "average_children_by_region.plot(kind = 'bar')\n",
    "average_children_by_region.plot(kind = 'bar', color = 'black', figsize=(15, 5))\n",
    "plt.title('Average # of Out-of-school Children of Primary School Age')\n",
    "plt.ylabel('# of Children')\n",
    "plt.xlabel('World Bank Regions')\n",
    "plt.xticks(rotation= 30)\n",
    "\n",
    "#group by year\n",
    "total_children_by_year = by_year['all_children'].sum()\n",
    "total_children_by_year.plot(kind = 'bar', color = 'black', figsize=(15, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#horizontal bar plot\n",
    "total_children_by_year.plot(kind = 'barh', color = 'black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#histogram\n",
    "df_merged['percentage_female'].hist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# boxplot\n",
    "pivot_female_percent_year_income.plot(kind = 'box', figsize=(15, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot\n",
    "pivot_year_region = df_merged.pivot_table('all_children',\n",
    "                     index = 'year', columns = 'Region', aggfunc= 'mean')\n",
    "pivot_year_region.plot(figsize=(15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatterplot\n",
    "total_female = df_merged[['all_children', 'percentage_female']]\n",
    "total_female.plot(kind ='scatter', x = 'all_children', y = 'percentage_female', figsize=(15, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ExtraTreesClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c22afd0a9d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtraTreesClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ExtraTreesClassifier'"
     ]
    }
   ],
   "source": [
    "# Feature-engineering \n",
    "\n",
    "#combining variables with the same unit and binning them\n",
    "\n",
    "import sklearn \n",
    "\n",
    "from sklearn.tree import ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Feature Engineering (Selection/Design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##data Classifier\n",
    "http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/classification.html\n",
    "http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/dataClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## # Feature selection   @@ Python @@\n",
    "clf = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf.fit(X_train, y_train)\n",
    "# clf.feature_importances_ \n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_test = fs.transform(X_test)\n",
    "test = fs.transform(test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sci-kit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "      #\n",
    "    TF-IDF\n",
    "    Word2Vec\n",
    "        Model\n",
    "        Example\n",
    "    StandardScaler\n",
    "        Model Fitting\n",
    "        Example\n",
    "    Normalizer\n",
    "        Example\n",
    "    ChiSqSelector\n",
    "        Model Fitting\n",
    "        Example\n",
    "    ElementwiseProduct\n",
    "        Example\n",
    "    PCA\n",
    "        Example\n",
    "\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Spark data feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the Numpy library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize \n",
    "#?? (maybe not a categorical)\n",
    "# perform z-score scaling\n",
    "features_mu = np.mean(features, axis=0)\n",
    "features_sigma = np.std(features, axis=0)\n",
    "std_features = (features - features_mu)/features_sigma\n",
    "\n",
    "\n",
    "# perform standard scaling again but via SciKit-Learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "z_scaler = StandardScaler()\n",
    "movie_features = z_scaler.fit_transform(movie_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nba_shot_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b70279974af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# split data into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_set_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.80\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnba_shot_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnba_shot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_set_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_Coordinate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y_Coordinate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnba_shot_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_set_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_Coordinate'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'y_Coordinate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nba_shot_data' is not defined"
     ]
    }
   ],
   "source": [
    "#split\n",
    "# split data into train and test\n",
    "train_set_size = int(.80*len(nba_shot_data))\n",
    "train_features = nba_shot_data.ix[:train_set_size,['x_Coordinate','y_Coordinate']].as_matrix()\n",
    "test_features = nba_shot_data.ix[train_set_size:,['x_Coordinate','y_Coordinate']].as_matrix()\n",
    "train_class_labels = nba_shot_data.ix[:train_set_size,['shot_outcome']].as_matrix()\n",
    "test_class_labels = nba_shot_data.ix[train_set_size:,['shot_outcome']].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#transform features so they are more representative  i.e. x,y to radians and radius\n",
    "\n",
    "\n",
    "#from cosine to linear \n",
    "z=np.sin(x+np.pi/2)\n",
    "\n",
    "## temperation during the day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# passing in the label to the algorihtm\n",
    "np.ravel(train_class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "methods take numpy arrays as inputs and therefore we will need to create those from the DataFrame that we already have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target: A one-dimensional numpy array containing the target/response from the train data.\n",
    "target = train[\"Survived\"].values\n",
    "labels = (movie_data['IMDB'] >= 7.).astype('int').tolist()\n",
    "\n",
    "# features: A multidimensional numpy array containing the features/predictors from the train data.\n",
    "features = train[[\"Sex\", \"Age\"]].values\n",
    "features = movie_data[['RottenTomatoes','RottenTomatoes_User','Metacritic','Metacritic_User','Fandango_Ratingvalue']].as_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### randomize (remove bais)\n",
    "\n",
    "?? np.random.shuffle(arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'arrange'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4bda4aded1cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0marr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'arrange'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## start_time = time.time()\n",
    "##model\n",
    "end_time = time.time()\n",
    "print \"Training ended after %.2f seconds.\" %(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Planning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Tools\n",
    "# python\n",
    "# R\n",
    "# Orange toolkit http://orange.biolab.si\n",
    "# H20\n",
    "# Spark\n",
    "# WEKA/WEKA\n",
    "# Wrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preventing Overfitting/Underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LASSO/ .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FukuML simple machine learning library"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://pypi.python.org/pypi/FukuML/0.1.4\n",
    "# Perceptron\n",
    "Perceptron Binary Classification Learning Algorithm\n",
    "Perceptron Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Perceptron Multi Classification Learning Algorithm\n",
    "Perceptron Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Pocket Perceptron Binary Classification Learning Algorithm\n",
    "Pocket Perceptron Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Pocket Perceptron Multi Classification Learning Algorithm\n",
    "Pocket Perceptron Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "\n",
    "# Regression\n",
    "Linear Regression Learning Algorithm\n",
    "Linear Regression Binary Classification Learning Algorithm\n",
    "Linear Regression Multi Classification Learning Algorithm\n",
    "Logistic Regression Learning Algorithm\n",
    "Logistic Regression Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression Binary Classification Learning Algorithm\n",
    "Logistic Regression Binary Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression One vs All Multi Classification Learning Algorithm\n",
    "Logistic Regression One vs All Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "Logistic Regression One vs One Multi Classification Learning Algorithm\n",
    "Logistic Regression One vs One Multi Classification Learning Algorithm with Linear Regression Accelerator\n",
    "\n",
    "# Classification\n",
    "https://www.datarobot.com/blog/classification-with-scikit-learn/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-7eb0e22c487e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-7eb0e22c487e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    naiveBayes.py\t= naive Bayes classifier. http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/naiveBayes.html\u001b[0m\n\u001b[0m                 \t            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "naiveBayes.py\t= naive Bayes classifier. http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/naiveBayes.html\n",
    "perceptron.py\t= perceptron classifier.  http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/perceptron.html\n",
    "mira.py\tMIRA classifier.   = http://www.cs.utexas.edu/~pstone/Courses/343spring12/assignments/classification/docs/mira.html\n",
    "Nilearn = http://nilearn.github.io/decoding/estimator_choice.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  https://github.com/dmlc/xgboost/tree/master/demo/multiclass_classification (Multi-class example X-GBoost)\n",
    "# Scikit-multi learn : http://scikit.ml\n",
    "# One versus All:\tsklearn.multiclass.OneVsRestClassifier\n",
    "# Multi-class and multi-label --http://scikit-learn.org/stable/modules/multiclass.html \n",
    "#http://scikit-learn.org/stable/modules/ensemble.html\n",
    "# http://scikit-learn.org/stable/modules/ensemble.html \n",
    "# http://www.codeproject.com/Articles/821347/MultiClass-Logistic-Classifier-in-Python (implementation in python)\n",
    "# multi-label http://scikit-learn.org/stable/auto_examples/plot_multilabel.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# set the random number generator for reproducability\n",
    "np.random.seed(123)\n",
    "\n",
    "# let's try to predict the IMDB rating from the others\n",
    "features = movie_data[['RottenTomatoes','RottenTomatoes_User','Metacritic','Metacritic_User','Fandango_Ratingvalue']].as_matrix()\n",
    "# create classes: more or less that 7/10 rating\n",
    "labels = (movie_data['IMDB'] >= 7.).astype('int').tolist()\n",
    "shuffle_in_unison(features, labels)\n",
    "\n",
    "### Your Code Goes Here ###\n",
    "\n",
    "# initialize and train a logistic regression model\n",
    "lm = LogisticRegression()\n",
    "lm.fit(features, labels)\n",
    "\n",
    "# compute error on training data\n",
    "predictions = lm.predict(features)\n",
    "train_error_rate = calc_classification_error(predictions, labels)\n",
    "\n",
    "###########################\n",
    "\n",
    "print \"Classification error on training set: %.2f%%\" %(train_error_rate*100)\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %((sum(labels)*100.)/len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_tree = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-f516a396281d>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-f516a396281d>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    .feature_importances_ #importance: requesting the\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#decision trees -- Import 'tree' from scikit-learn library\n",
    "from sklearn import tree\n",
    "\n",
    "#training\n",
    "\n",
    "my_tree = tree.DecisionTreeClassifier()\n",
    "my_tree = my_tree.fit(features, target)\n",
    "\n",
    "#analysis & testing:\n",
    "\n",
    ".feature_importances_ #importance: requesting the \n",
    ".score() # mean accuracy that you can compute using the\n",
    "\n",
    "    print(my_tree_one.feature_importances_)\n",
    "print(my_tree_one.score(features_one, target))\n",
    "clf.score(features_test,labels_test)\n",
    "\n",
    "#control overfitting\n",
    "# DecisionTreeRegressor\n",
    "#, the depth of our model is defined by two parameters: \n",
    "#    - the max_depth parameter determines when the splitting up of the decision tree stops. \n",
    "#    - the min_samples_split parameter monitors the amount of observations in a bucket. \n",
    "#If a certain threshold is not reached (e.g minimum 10 passengers) no further splitting can be done.\n",
    "#By limiting the complexity of your decision tree you will increase its generality and thus its usefulness for prediction!\n",
    "\n",
    "# Create a new array with the added features: features_two\n",
    "features_two = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "# Control overfitting by setting \"max_depth\" to 10 and \"min_samples_split\" to 5 : my_tree_two\n",
    "max_depth = 10\n",
    "min_samples_split = 5\n",
    "my_tree_two = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 5, random_state = 1)\n",
    "\n",
    "\n",
    "#2-D plotting\n",
    "%pylab inline\n",
    "import sys #?\n",
    "import matplotlib.pyplot as plt #?\n",
    "import numpy as np #?\n",
    "import pylab as pl #?\n",
    "sys.path.append('../tools') #?\n",
    "Populating the interactive namespace from numpy and matplotlib #?\n",
    "from class_vis import prettyPicture, output_image\n",
    "from prep_terrain_data import makeTerrainData\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test)\n",
    "# output_image(\"test.png\", \"png\", open(\"test.png\", \"rb\").read())\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import ensemble\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "## # Feature selection   @@ Python @@\n",
    "clf = ExtraTreesClassifier(random_state=1729)\n",
    "selector = clf.fit(X_train, y_train)\n",
    "# clf.feature_importances_ \n",
    "fs = SelectFromModel(selector, prefit=True)\n",
    "\n",
    "X_train = fs.transform(X_train)\n",
    "X_test = fs.transform(X_test)\n",
    "test = fs.transform(test)\n",
    "\n",
    "print(X_train.shape, X_test.shape, test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Random Forest \n",
    "#handles the overfitting problem you faced with decision trees. \n",
    "#grows multiple (very deep) classification trees using the training set.\n",
    "# each tree is used to come up with a prediction and every outcome is counted as a vote\n",
    "# majority's vote count as the actual classification decision, avoids overfitting.\n",
    "\n",
    "#n_estimators=allows you to set the number of trees you wish to plant and average over.\n",
    "\n",
    "\n",
    "#Import the `RandomForestClassifier`\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#We want the Pclass, Age, Sex, Fare,SibSp, Parch, and Embarked variables\n",
    "features_forest = train[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "\n",
    "#Building the Forest: my_forest\n",
    "n_estimators = 100\n",
    "forest = RandomForestClassifier(max_depth = 10, min_samples_split=2, n_estimators=100, random_state = 1)\n",
    "my_forest = forest.fit(features_forest, target)\n",
    "\n",
    "#Print the score of the random forest\n",
    "print(my_forest.score(features_forest, target))\n",
    "\n",
    "#Compute predictions and print the length of the prediction vector:test_features, pred_forest\n",
    "test_features = test[[\"Pclass\",\"Age\",\"Sex\",\"Fare\", \"SibSp\", \"Parch\", \"Embarked\"]].values\n",
    "pred_forest = my_forest.predict(test_features)\n",
    "print(len(pred_forest))\n",
    "\n",
    "\n",
    "#feature importances and score\n",
    "\n",
    "#Request and print the `.feature_importances_` attribute\n",
    "print(my_tree_two.feature_importances_)\n",
    "print(my_forest.feature_importances_)\n",
    "\n",
    "#Compute and print the mean accuracy score for both models\n",
    "print(my_tree_two.score(features_two, target))\n",
    "print(my_forest.score(features_two, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-e5a6372a47b0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-e5a6372a47b0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    http://napitupulu-jon.appspot.com/posts/decision-tree-ud.html\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test mining\n",
    "http://napitupulu-jon.appspot.com/posts/decision-tree-ud.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emsemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "http://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python-Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Numpy-Linear Algebra/Arrays\n",
    "\n",
    "http://scipy.github.io/old-wiki/pages/Tentative_NumPy_Tutorial.html#A.22Automatic.22_Reshaping \n",
    "\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# you can create a 1-d array with a list of numbers\n",
    "a = np.array([1, 4, 6])\n",
    "print np.ones((3, 4))\n",
    "print np.zeros((2, 5))\n",
    "\n",
    "#Transpose matrix\n",
    "print b.T\n",
    "\n",
    "#Inverse a matrix\n",
    "m_inverse = np.linalg.inv(m)\n",
    "\n",
    "# you can convert a 1-d array to a 2-d array with np.newaxis\n",
    "(3,)-> a[np.newaxis].shape: (1, 3)\n",
    "\n",
    "#reshape\n",
    "\n",
    "#concate arrays\n",
    "np.hstack([b, b]):\n",
    "np.vstack([b, b]):\n",
    "\n",
    "#matrix multiplication\n",
    "# you can perform matrix multiplication with np.dot()\n",
    "c = np.dot(a, b)\n",
    "\n",
    "# you can perform element-wise multiplication with * \n",
    "d = b * b\n",
    "\n",
    "#decision stuctures\n",
    "\n",
    "for this_row in b:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Pandas-Scientific Computing/Data Frames\n",
    "http://pandas-docs.github.io/pandas-docs-travis/ \n",
    "\n",
    "# Create a dataframe\n",
    "df = pd.DataFrame(data=b,  columns=['Weight', 'Height'])\n",
    "\n",
    "\n",
    "# read .csv files\n",
    "baseball = pd.read_csv('data/baseball.dat.txt')\n",
    "\n",
    "# save .csv files\n",
    "\n",
    "\n",
    "## expore data\n",
    "\n",
    "baseball.head()  \t#first five rows\n",
    "baseball.keys()  \t\t# column name, and data\n",
    "baseball.info()\t\t\t# column data type, and no. of rows \n",
    "Salary                     337 non-null int64\n",
    "baseball.describe()\t\t#summary of mean, sd, count and percentiles\n",
    "\n",
    "# Query data\n",
    "millionaire_indices = baseball['Salary'] > 1000\t\t\t\t\t   # filter\n",
    "print (\"baseball[millionaire_indices].shape:\", baseball[millionaire_indices].shape)    # by column\n",
    "baseball[millionaire_indices][['Salary', 'AVG', 'Runs', 'Name']]\t                   #subset by column\n",
    "\n",
    "\n",
    "#joins\n",
    "merged = pd.merge(baseball, shoe_size_df, on=['Name'])\n",
    "merged_outer = pd.merge(baseball, shoe_sizes, on=['Name'], how='outer')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "decision-tree-ud_files/4.jpg",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Presentation\n",
    "\n",
    "#display image from pc\n",
    "from IPython.display import Image\n",
    "Image('decision-tree-ud_files/4.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Other Datasets\n",
    "- [World Bank Data](http://data.worldbank.org/)\n",
    "- [United Nations Data](http://data.un.org/)\n",
    "- [Toronto Open Data](http://www1.toronto.ca/wps/portal/contentonly?vgnextoid=1a66e03bb8d1e310VgnVCM10000071d60f89RCRD)\n",
    "- [Reddit /r/datasets](reddit.com/r/datasets)\n",
    "- [100+ Interesting Datasets](http://rs.io/100-interesting-data-sets-for-statistics/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
